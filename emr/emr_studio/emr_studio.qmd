---
title: "EMR Studio"
format: html
editor: visual
---

## Run EMR Studio Notebook

### Start Cluster

```{bash}

aws emr create-cluster \
 --name "pgx-analysis-ascpt" \
 --log-uri "s3://athena-output-testing-pgx/elasticmapreduce" \
 --release-label "emr-7.5.0" \
 --service-role "arn:aws:iam::535362115856:role/EMR_DefaultRole" \
 --security-configuration "pgx_emr2" \
 --managed-scaling-policy '{"ComputeLimits":{"UnitType":"Instances","MinimumCapacityUnits":3,"MaximumCapacityUnits":100,"MaximumOnDemandCapacityUnits":100,"MaximumCoreCapacityUnits":100}}' \
 --unhealthy-node-replacement \
 --ec2-attributes '{"InstanceProfile":"EMR_EC2_DefaultRole","EmrManagedMasterSecurityGroup":"sg-0a5260eee9a20654e","EmrManagedSlaveSecurityGroup":"sg-0f01580b24ea364e1","KeyName":"mushin_pgx","AdditionalMasterSecurityGroups":[],"AdditionalSlaveSecurityGroups":[],"ServiceAccessSecurityGroup":"sg-0e25102a082ef38d1","SubnetId":"subnet-03f689fd2cb8cbb2c"}' \
 --tags 'creator=CREATE_CLUSTER_TAB_IN_JUPYTERLAB_PLUGIN' \
 --applications Name=Hadoop Name=JupyterEnterpriseGateway Name=Livy Name=Spark \
 --configurations '[{"Classification":"livy-conf","Properties":{"livy.server.session.timeout":"6h","livy.spark.deploy-mode":"cluster"}},{"Classification":"spark-hive-site","Properties":{"hive.metastore.client.factory.class":"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"}}]' \
 --instance-groups '[{"InstanceCount":1,"InstanceGroupType":"MASTER","Name":"master","InstanceType":"m2.2xlarge","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"VolumeType":"gp2","SizeInGB":32},"VolumesPerInstance":2}],"EbsOptimized":false}},{"InstanceCount":3,"InstanceGroupType":"CORE","Name":"slave","InstanceType":"m2.2xlarge","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"VolumeType":"gp2","SizeInGB":32},"VolumesPerInstance":2}],"EbsOptimized":false}}]' \
 --bootstrap-actions '[{"Args":[],"Name":"pgx","Path":"s3://pgx-repository/emr-config/pgx_emr_studio_python39_bootstrap.sh"}]' \
 --auto-scaling-role "arn:aws:iam::535362115856:role/EMR_AutoScaling_DefaultRole" \
 --scale-down-behavior "TERMINATE_AT_TASK_COMPLETION" \
 --ebs-root-volume-size "97" \
 --auto-termination-policy '{"IdleTimeout":3600}' \
 --region "us-east-1" \
 --profile pgx

```

### Studio ID

```{bash}

STUDIO_ID=$(aws emr list-studios --profile pgx --output json | jq -r '.Studios[0].StudioId')
export STUDIO_ID
echo "Studio ID: $STUDIO_ID"

```


```{bash}

aws emr describe-studio --studio-id $STUDIO_ID --profile pgx

```
### Workspace ID (AWS Console)

```{bash}

workspace_id = 'e-EBO8QCTW0J9YYPNH4RG8Q247P'

```


### Cluster ID

```{bash}

# Set the working directory
CWD=$(pwd)

# Get EMR cluster info and save it to a JSON file
aws emr list-clusters --active --profile pgx | tee "${CWD}/emr/cluster-config/cluster_info.json"

# Parse the cluster ID from the JSON file using jq
CLUSTER_ID=$(jq -r '.Clusters[0].Id' "${CWD}/emr/cluster-config/cluster_info.json")

# Export the cluster ID as an environment variable
export CLUSTER_ID="$CLUSTER_ID"

# Output the cluster ID for verification
echo "Cluster ID: $CLUSTER_ID"


```

### Start Notebook Execution with Parameters

```{bash}

aws emr start-notebook-execution \
  --editor-id $workspace_id \
  --notebook-params '{"cohort" : "cohort1", "cohort_num" : 1}' \
  --relative-path pipeline/0_cohort_dataset_builder.ipynb \
  --notebook-execution-name BUILD_DS \
  --execution-engine "{\"Id\": \"$CLUSTER_ID\"}" \
  --service-role EMRStudio_Service_Role \
  --profile pgx


```


```{bash}

# Retrieve Notebook Execution ID
NOTEBOOK_EXECUTION_ID=$(aws emr list-notebook-executions --studio-id "$STUDIO_ID" --profile pgx --output json | jq -r '.NotebookExecutions[0].NotebookExecutionId')
export NOTEBOOK_EXECUTION_ID
echo "Notebook Execution ID: $NOTEBOOK_EXECUTION_ID"

# Describe the Notebook Execution
if [ -n "$NOTEBOOK_EXECUTION_ID" ]; then
  aws emr describe-notebook-execution --notebook-execution-id "$NOTEBOOK_EXECUTION_ID" --profile pgx
else
  echo "No Notebook Execution ID found for Studio ID: $STUDIO_ID"
fi

```

### Sync AWS EMR Studio Notebooks with GitHub

```{python}

import boto3
from github import Github
import io
import hashlib

# AWS S3 Configuration
S3_BUCKET = "emr-studio-dependencies-emrstudiostoragebucket-r84qxyqp141i"

# GitHub Configuration
GITHUB_TOKEN = "your_github_token"  # Replace with your GitHub token
REPO_NAME = "Jerome3590/drug_feature_importance_pipeline"
GITHUB_FOLDER = "notebook_pipeline"  # Folder in the repo

# Initialize AWS S3 client
s3 = boto3.client("s3")

# Initialize GitHub client
g = Github(GITHUB_TOKEN)
repo = g.get_repo(REPO_NAME)

def calculate_hash(content):
    """Calculate SHA256 hash of the content."""
    return hashlib.sha256(content.encode("utf-8")).hexdigest()

# Step 1: List .ipynb files in the S3 bucket
response = s3.list_objects_v2(Bucket=S3_BUCKET)

for obj in response.get("Contents", []):
    key = obj["Key"]
    if key.endswith(".ipynb"):
        print(f"Processing {key} from S3")

        # Step 2: Read file content into an in-memory buffer
        buffer = io.BytesIO()
        s3.download_fileobj(S3_BUCKET, key, buffer)
        buffer.seek(0)
        notebook_content = buffer.read().decode("utf-8")  # Decode as text

        # Step 3: Prepare the GitHub file path
        file_name = key.split("/")[-1]
        file_repo_path = f"{GITHUB_FOLDER}/{file_name}"

        # Step 4: Check if the file exists in the GitHub repository
        try:
            existing_file = repo.get_contents(file_repo_path)
            existing_hash = calculate_hash(existing_file.decoded_content.decode("utf-8"))
            current_hash = calculate_hash(notebook_content)

            # Skip upload if the file has not changed
            if existing_hash == current_hash:
                print(f"Skipping {file_repo_path}: No changes detected")
                continue

            # Update the file if it has changed
            print(f"Updating {file_repo_path} in the repository")
            repo.update_file(
                path=file_repo_path,
                message=f"Update notebook: {file_name}",
                content=notebook_content,
                sha=existing_file.sha,
            )
        except Exception as e:
            # Create the file if it doesn't exist
            print(f"Creating {file_repo_path} in the repository")
            repo.create_file(
                path=file_repo_path,
                message=f"Add notebook: {file_name}",
                content=notebook_content,
            )

print("Sync completed.")


```

