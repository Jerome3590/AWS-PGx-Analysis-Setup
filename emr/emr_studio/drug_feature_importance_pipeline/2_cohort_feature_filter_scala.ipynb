{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7023248b-5bac-4c7f-a895-70d667ea7748",
   "metadata": {},
   "source": [
    "# Model Feature Importance Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733684f-e44b-4342-bc0d-6d6deab7f047",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "val cohort = \"cohort1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8806e7-2c4a-4e1e-870b-50319ab055b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{StructType, StructField, ArrayType, StringType}\n",
    "import ai.catboost.spark.{CatBoostClassificationModel, Pool}\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().\n",
    "  appName(\"Evaluate Multiple CatBoost Models For Feature Importance\").\n",
    "  config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.5_2.12:1.2.7\").\n",
    "  config(\"spark.executor.memory\", \"24g\").\n",
    "  config(\"spark.executor.memoryOverhead\", \"4g\").\n",
    "  config(\"spark.executor.cores\", \"4\").\n",
    "  config(\"spark.driver.memory\", \"24g\").\n",
    "  getOrCreate()\n",
    "\n",
    "\n",
    "// S3 paths\n",
    "val modelBasePath = s\"s3a://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/4_models/${cohort}/spark_model_\"\n",
    "val featureImportanceBasePath = s\"s3a://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/5_feature_importances/$cohort/feature_importance_model_\"\n",
    "val s3_bucket = s\"s3a://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/2_processed_datasets/${cohort}\"\n",
    "val drug_name_path = s\"${s3_bucket}/feature_info_${cohort}.json\"\n",
    "\n",
    "\n",
    "// Define schema for drug names\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"names\", ArrayType(StringType), true),\n",
    "  StructField(\"types\", StringType, true),\n",
    "  StructField(\"source_column\", StringType, true)\n",
    "))\n",
    "\n",
    "// Read drug names for filtering\n",
    "val drug_names_json = spark.read.schema(schema).option(\"multiline\", \"true\").json(drug_name_path)\n",
    "\n",
    "// Process drug names\n",
    "val drug_name_df = drug_names_json.select(explode(col(\"names\")).alias(\"raw_name\")).\n",
    "  select(\n",
    "    trim(split(col(\"raw_name\"), \"\\\\|\")(0)).alias(\"drug_name\"),   // Extract drug name\n",
    "    trim(split(col(\"raw_name\"), \"\\\\|\")(1)).alias(\"drug_index\")  // Extract drug index\n",
    "  ).\n",
    "  withColumn(\"drug_index\", regexp_replace(col(\"drug_index\"), \"drug_name_index_\", \"\").cast(\"int\")).\n",
    "  orderBy(col(\"drug_index\"))\n",
    "\n",
    "// Loop through all 10 models\n",
    "for (modelNum <- 1 to 10) {\n",
    "  // Load the CatBoost model\n",
    "  val modelPath = s\"${modelBasePath}${modelNum}\"\n",
    "  val catBoostModel = CatBoostClassificationModel.load(modelPath)\n",
    "\n",
    "  // Get feature importances\n",
    "  val featureImportances = catBoostModel.getFeatureImportance()\n",
    "\n",
    "  // Create a DataFrame with feature importances\n",
    "  val importanceDf = spark.createDataFrame(featureImportances.zipWithIndex.map { case (importance, index) =>\n",
    "    (index, importance.toDouble)\n",
    "  }).toDF(\"array_index\", \"importance\").filter(col(\"importance\") =!= 0)\n",
    "\n",
    "  // Join feature importances with drug names\n",
    "  val joinedDf = drug_name_df.join(importanceDf, drug_name_df(\"drug_index\") === importanceDf(\"array_index\"), \"inner\").\n",
    "    select(\"drug_name\", \"drug_index\", \"importance\").\n",
    "    orderBy(col(\"importance\").desc)\n",
    "\n",
    "  // Define output paths\n",
    "  val tempOutputDir = s\"s3a://pgx-repository/temp_model_${modelNum}\"\n",
    "  val finalOutputPath = s\"${featureImportanceBasePath}${modelNum}.csv\"\n",
    "\n",
    "  // Save to temporary location\n",
    "  val fs = FileSystem.get(new java.net.URI(\"s3a://pgx-repository\"), spark.sparkContext.hadoopConfiguration)\n",
    "  joinedDf.coalesce(1).write.\n",
    "    option(\"header\", \"true\").\n",
    "    mode(\"overwrite\").\n",
    "    csv(tempOutputDir)\n",
    "\n",
    "  // Locate and move the part file\n",
    "  val partFile = fs.listStatus(new Path(tempOutputDir)).\n",
    "    find(_.getPath.getName.startsWith(\"part-\")).\n",
    "    map(_.getPath).\n",
    "    getOrElse(throw new RuntimeException(s\"Part file not found for model $modelNum\"))\n",
    "\n",
    "  val finalPath = new Path(finalOutputPath)\n",
    "  if (fs.exists(finalPath)) fs.delete(finalPath, false)\n",
    "  fs.rename(partFile, finalPath)\n",
    "\n",
    "  println(s\"Feature importance with drug names for $cohort, model $modelNum successfully written to $finalOutputPath\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a773ea7-ac10-4016-9e0f-034ae43030af",
   "metadata": {},
   "source": [
    "## Consolidated Feature Importance for Dataset Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f9b1f-d657-4625-87b8-8062954b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "cohort = \"cohort1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f180c5-3bd7-4c9e-96fd-56a68f24fcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from collections import defaultdict\n",
    "\n",
    "# AWS S3 bucket and prefix (folder) details\n",
    "bucket_name = \"pgx-repository\"\n",
    "feature_importance_prefix = f\"ade-risk-model/Step5_Time_to_Event_Model/5_feature_importances/{cohort}/\"\n",
    "consolidated_output_path = \"consolidated_feature_importances.csv\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# List all CSV files in the specified S3 directory\n",
    "csv_files = []\n",
    "\n",
    "# Dictionary to store feature counts and importance values\n",
    "feature_importance_dict = defaultdict(list)\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=feature_importance_prefix)\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].endswith(\".csv\"):\n",
    "            csv_files.append(obj[\"Key\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing S3 bucket: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Found {len(csv_files)} model feature importance files in S3.\")\n",
    "\n",
    "# Loop through each CSV file and process feature importances\n",
    "for s3_key in csv_files:\n",
    "    print(f\"Processing file: {s3_key}\")\n",
    "    try:\n",
    "        # Read the file content from S3\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "        # Validate column names\n",
    "        if 'drug_name' not in df.columns or 'importance' not in df.columns:\n",
    "            print(f\"Skipping {s3_key}: 'drug_name' or 'importance' column missing.\")\n",
    "            continue\n",
    "\n",
    "        # Filter features with non-zero importance\n",
    "        important_features = df[df['importance'] > 0][['drug_name', 'importance']]\n",
    "\n",
    "        # Update the dictionary with importance values\n",
    "        for _, row in important_features.iterrows():\n",
    "            feature_importance_dict[row['drug_name']].append(row['importance'])\n",
    "\n",
    "        print(f\"Processed {s3_key}: {len(important_features)} important features found.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {s3_key}: {e}\")\n",
    "\n",
    "# Build the consolidated DataFrame\n",
    "consolidated_data = []\n",
    "for feature, importances in feature_importance_dict.items():\n",
    "    consolidated_data.append({\n",
    "        'drug_name': feature,\n",
    "        'num_matches': len(importances),\n",
    "        'mean_importance': sum(importances) / len(importances)  \n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame\n",
    "consolidated_df = pd.DataFrame(consolidated_data)\n",
    "\n",
    "# Sort first by num_matches (descending) and then by mean_importance (descending)\n",
    "consolidated_df = consolidated_df.sort_values(by=['num_matches', 'mean_importance'], ascending=[False, False])\n",
    "\n",
    "# Save the consolidated feature importance to S3\n",
    "try:\n",
    "    csv_buffer = io.StringIO()\n",
    "    consolidated_df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=feature_importance_prefix + consolidated_output_path, Body=csv_buffer.getvalue())\n",
    "    print(f\"\\nConsolidated feature importance saved to S3 at: {feature_importance_prefix + consolidated_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving consolidated file to S3: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nTop consolidated features across models:\")\n",
    "print(consolidated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf2346-4bfd-4a03-98b2-a5b3795ef392",
   "metadata": {},
   "source": [
    "### Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57967b79-87aa-4c9b-9370-4707f08c5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "cohort_num = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea455b0-f264-42f3-b62e-89b853bd06a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Set the backend before importing pyplot\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# AWS S3 bucket and prefix (folder) details\n",
    "bucket_name = \"pgx-repository\"\n",
    "s3_key = f\"ade-risk-model/Step5_Time_to_Event_Model/5_feature_importances/cohort{cohort_num}/consolidated_feature_importances.csv\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Read consolidated_df\n",
    "obj = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "consolidated_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "# Combine num_matches and mean_importance for clustering\n",
    "clustering_data = consolidated_df[['num_matches', 'mean_importance']]\n",
    "\n",
    "# Function to find the optimal number of clusters using WSS\n",
    "def find_optimal_clusters(data, max_k=10):\n",
    "    wss = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=25, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        wss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Create a Plotly line plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(1, max_k + 1)),\n",
    "        y=wss,\n",
    "        mode='lines+markers',\n",
    "        name='WSS'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=\"Elbow Method for Optimal k\",\n",
    "        xaxis_title=\"Number of Clusters (k)\",\n",
    "        yaxis_title=\"Within-Cluster Sum of Squares (WSS)\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Automatically detect the \"elbow\" using kneed\n",
    "    kneedle = KneeLocator(range(1, max_k + 1), wss, curve=\"convex\", direction=\"decreasing\")\n",
    "    optimal_k = kneedle.knee\n",
    "    if optimal_k is None:\n",
    "        raise ValueError(\"Unable to detect the elbow point. Please check the data.\")\n",
    "    print(f\"Optimal number of clusters (k): {optimal_k}\")\n",
    "    return optimal_k\n",
    "\n",
    "optimal_k = find_optimal_clusters(clustering_data)\n",
    "kmeans = KMeans(n_clusters=optimal_k, n_init=25, random_state=42)\n",
    "consolidated_df['Cluster'] = kmeans.fit_predict(clustering_data)\n",
    "\n",
    "cluster_df = consolidated_df.sort_values(by=['num_matches', 'mean_importance', 'Cluster'], ascending=[False, False, False])\n",
    "\n",
    "# Display the clustered DataFrame\n",
    "print(\"Clustered DataFrame:\")\n",
    "print(cluster_df)\n",
    "\n",
    "# Save the clustered data to a CSV file\n",
    "csv_buffer = io.StringIO()\n",
    "cluster_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload clustered data to S3\n",
    "s3_client.put_object(\n",
    "    Bucket=\"pgx-repository\",\n",
    "    Key=f\"ade-risk-model/Step5_Time_to_Event_Model/6_feature_filters/cohort{cohort_num}_clustered_features.csv\",\n",
    "    Body=csv_buffer.getvalue()\n",
    ")\n",
    "print(\"Clustered data uploaded to S3 successfully.\")\n",
    "\n",
    "# Plot clusters using Plotly Express\n",
    "fig = px.scatter(\n",
    "    cluster_df,\n",
    "    x='num_matches',\n",
    "    y='mean_importance',\n",
    "    color='Cluster',\n",
    "    text='drug_name',\n",
    "    title=f\"Cohort {cohort_num}: Clusters of Features Based on Matches and Mean Importance\",\n",
    "    labels={\"num_matches\": \"Number of Matches\", \"mean_importance\": \"Mean Importance\"}\n",
    ")\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "# Save the interactive plot to an HTML file\n",
    "html_buffer = io.StringIO()\n",
    "fig.write_html(html_buffer)\n",
    "\n",
    "# Upload the Plotly HTML plot to S3\n",
    "s3_client.put_object(\n",
    "    Bucket=\"pgx-repository\",\n",
    "    Key=f\"ade-risk-model/Step5_Time_to_Event_Model/6_feature_filters/cohort{cohort_num}_clustered_features_plot.html\",\n",
    "    Body=html_buffer.getvalue(),\n",
    "    ContentType=\"text/html\"\n",
    ")\n",
    "print(\"Interactive Plotly plot uploaded to S3 successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474130bf-2c12-41a2-be85-b74233f97b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
