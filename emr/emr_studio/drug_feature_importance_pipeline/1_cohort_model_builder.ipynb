{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d5dedf-e7c6-476d-a86e-34eb13b5f870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T01:34:22.510746Z",
     "iopub.status.busy": "2024-12-25T01:34:22.510074Z",
     "iopub.status.idle": "2024-12-25T01:34:22.544602Z",
     "shell.execute_reply": "2024-12-25T01:34:22.543596Z",
     "shell.execute_reply.started": "2024-12-25T01:34:22.510714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'ai.catboost:catboost-spark_3.5_2.12:1.2.7', 'spark.executor.memory': '24g', 'spark.executor.cores': '4', 'spark.driver.memory': '24g', 'spark.yarn.am.memory': '4g', 'spark.dynamicAllocation.enabled': 'true', 'spark.task.cpus': '4', 'spark.jars.packages.resolve.transitive': 'true', 'spark.executor.extraJavaOptions': '--add-exports java.base/sun.net.util=ALL-UNNAMED', 'spark.driver.extraJavaOptions': '--add-exports java.base/sun.net.util=ALL-UNNAMED', 'spark.network.timeout': '1200s', 'spark.rpc.askTimeout': '1200s', 'spark.executor.memoryOverhead': '4g'}, 'proxyUser': 'assumed-role_EMRStudio_User_Role_jdixon3590', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1735086121891_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-15-179.ec2.internal:20888/proxy/application_1735086121891_0003/\" class=\"emr-proxy-link j-143D73CQMUEDN application_1735086121891_0003\" emr-resource=\"j-143D73CQMUEDN\n\" application-id=\"application_1735086121891_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-7-97.ec2.internal:8042/node/containerlogs/container_1735086121891_0003_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"ai.catboost:catboost-spark_3.5_2.12:1.2.7\",\n",
    "        \"spark.executor.memory\": \"24g\",\n",
    "        \"spark.executor.cores\": \"4\",       \n",
    "        \"spark.driver.memory\": \"24g\",      \n",
    "        \"spark.yarn.am.memory\": \"4g\",     \n",
    "        \"spark.dynamicAllocation.enabled\": \"true\", \n",
    "        \"spark.task.cpus\": \"4\",          \n",
    "        \"spark.jars.packages.resolve.transitive\": \"true\",\n",
    "        \"spark.executor.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.driver.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.network.timeout\": \"1200s\",  \n",
    "        \"spark.rpc.askTimeout\": \"1200s\", \n",
    "        \"spark.executor.memoryOverhead\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba04fa74-e410-455e-ba84-919cbeefe66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T01:34:27.322604Z",
     "iopub.status.busy": "2024-12-25T01:34:27.322301Z",
     "iopub.status.idle": "2024-12-25T01:36:27.500021Z",
     "shell.execute_reply": "2024-12-25T01:36:27.498803Z",
     "shell.execute_reply.started": "2024-12-25T01:34:27.322574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612c9fcec8224cb9922e844e7d05fe35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "\n",
      "stderr: \n",
      "\tfound commons-io#commons-io;2.7 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.11 in central\n",
      "\tfound org.apache.commons#commons-text;1.10.0 in central\n",
      "\tfound org.json4s#json4s-jackson_2.12;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-core_2.12;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-ast_2.12;3.7.0-M11 in central\n",
      "\tfound org.json4s#json4s-scalap_2.12;3.7.0-M11 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.2 in central\n",
      "\tfound io.github.classgraph#classgraph;4.8.98 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound ai.catboost#catboost-common;1.2.7 in central\n",
      "\tfound javax.validation#validation-api;1.1.0.Final in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound ai.catboost#catboost-spark-macros_2.12;1.2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.12 in central\n",
      ":: resolution report :: resolve 861ms :: artifacts dl 50ms\n",
      "\t:: modules in use:\n",
      "\tai.catboost#catboost-common;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark-macros_2.12;1.2.7 from central in [default]\n",
      "\tai.catboost#catboost-spark_3.5_2.12;1.2.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;32.0.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;2.8 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-io#commons-io;2.7 from central in [default]\n",
      "\tio.github.classgraph#classgraph;4.8.98 from central in [default]\n",
      "\tjavax.validation#validation-api;1.1.0.Final from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.11 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.10.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.33.0 from central in [default]\n",
      "\torg.json4s#json4s-ast_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-core_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-jackson_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.json4s#json4s-scalap_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.12 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.6.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-lang3;3.12.0 by [org.apache.commons#commons-lang3;3.11] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 by [com.fasterxml.jackson.core#jackson-databind;2.15.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   30  |   0   |   0   |   2   ||   28  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a6f047f1-43ec-4d67-991a-aba09b0e9d49\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 28 already retrieved (0kB/18ms)\n",
      "24/12/25 01:34:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/25 01:34:31 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-15-179.ec2.internal/172.31.15.179:8032\n",
      "24/12/25 01:34:32 INFO Configuration: resource-types.xml not found\n",
      "24/12/25 01:34:32 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/12/25 01:34:32 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (30720 MB per container)\n",
      "24/12/25 01:34:32 INFO Client: Will allocate AM container, with 27033 MB memory including 2457 MB overhead\n",
      "24/12/25 01:34:32 INFO Client: Setting up container launch context for our AM\n",
      "24/12/25 01:34:32 INFO Client: Setting up the launch environment for our AM container\n",
      "24/12/25 01:34:32 INFO Client: Preparing resources for our AM container\n",
      "24/12/25 01:34:32 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/25 01:34:35 INFO Client: Uploading resource file:/mnt/tmp/spark-470361cd-6ddd-42e0-96fd-f429c667a921/__spark_libs__18356131127458862599.zip -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/__spark_libs__18356131127458862599.zip\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/kryo-shaded-4.0.2.jar\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.8.0-incubating.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/livy-api-0.8.0-incubating.jar\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/livy-rsc-0.8.0-incubating.jar\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.8.0-incubating.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/livy-thriftserver-session-0.8.0-incubating.jar\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/minlog-1.3.0.jar\n",
      "24/12/25 01:34:42 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-all-4.1.100.Final.jar\n",
      "24/12/25 01:34:43 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-buffer-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-buffer-4.1.100.Final.jar\n",
      "24/12/25 01:34:43 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-4.1.100.Final.jar\n",
      "24/12/25 01:34:43 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-dns-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-dns-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-haproxy-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-haproxy-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-http-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http2-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-http2-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-memcache-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-memcache-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-mqtt-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-mqtt-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-redis-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-redis-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-smtp-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-smtp-4.1.100.Final.jar\n",
      "24/12/25 01:34:44 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-socks-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-socks-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-stomp-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-stomp-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-xml-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-codec-xml-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-common-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-common-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-handler-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-proxy-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-handler-proxy-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-handler-ssl-ocsp-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-resolver-4.1.100.Final.jar\n",
      "24/12/25 01:34:45 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-4.1.100.Final.jar -> hdfs://ip-172-31-15-179.ec2.internal:8020/user/livy/.sparkStaging/application_1735086121891_0005/netty-resolver-dns-4.1.100.Final.jar.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "import catboost_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7407cc-8efe-4fe0-8003-0a06d020be06",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding a parameter tag\n",
    "cohort = 'cohort1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe237a-df8a-47fe-805d-1c91368c5f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 Paths\n",
    "s3_bucket = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/2_processed_datasets/{cohort}\"\n",
    "train_input_path = f\"{s3_bucket}/train\"\n",
    "test_input_path = f\"{s3_bucket}/test\"\n",
    "\n",
    "# Read processed train and test datasets from S3\n",
    "print(\"Reading train and test datasets...\")\n",
    "train_df = spark.read.parquet(train_input_path)\n",
    "test_df = spark.read.parquet(test_input_path)\n",
    "\n",
    "print(\"Train and test datasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b8757-0c37-4dc0-9337-410dc9616c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify output\n",
    "print(\"Train Dataframe Schema:\")\n",
    "train_df.printSchema()\n",
    "print(\"Test Dataframe Schema:\")\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe3820-8c8b-41a2-a92d-3331f7086a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoost Pool objects\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache or persist the Spark DataFrames before creating the Pool\n",
    "train_df = train_df.select(\"features\", \"label\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df = test_df.select(\"features\", \"label\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Create the CatBoost Pool objects\n",
    "train_pool = catboost_spark.Pool(train_df)\n",
    "test_pool = catboost_spark.Pool(test_df)\n",
    "\n",
    "# Confirm the DataFrames are cached/persisted\n",
    "print(train_df.storageLevel)\n",
    "print(test_df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d92489-4702-4c25-83d1-f129d1cbcbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seeds for different runs\n",
    "seeds = [97, 11, 35, 90, 38, 74, 25, 974]\n",
    "\n",
    "# Start model number tracker\n",
    "model_num = 3\n",
    "\n",
    "# Loop to train and save models (10 runs for stable feature selection)\n",
    "for seed in seeds:\n",
    "    print(f\"Training model {model_num} with seed {seed}...\")\n",
    "    \n",
    "    # Initialize CatBoost Spark Classifier with the current seed\n",
    "    classifier = catboost_spark.CatBoostClassifier(randomSeed=seed)\n",
    "\n",
    "    # Train the model\n",
    "    model = classifier.fit(train_pool, evalDatasets=[test_pool])\n",
    "\n",
    "    # Define the path to save the Spark model, including the model number\n",
    "    spark_model_path = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/4_models/{cohort}/spark_model_{model_num}\"\n",
    "\n",
    "    # Save the Spark model (with metadata)\n",
    "    model.write().overwrite().save(spark_model_path)\n",
    "\n",
    "    print(f\"Spark model {model_num} with metadata saved to: {spark_model_path}\")\n",
    "    \n",
    "    # Increment the model number for the next run\n",
    "    model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b8922-5a40-4728-ac0c-7803e78f49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.unpersist()\n",
    "test_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994ae6d-b4ba-4622-9bcb-cc119f29e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
