{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5dedf-e7c6-476d-a86e-34eb13b5f870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"ai.catboost:catboost-spark_3.5_2.12:1.2.7\",\n",
    "        \"spark.executor.memory\": \"80g\",\n",
    "        \"spark.executor.cores\": \"32\",       \n",
    "        \"spark.driver.memory\": \"20g\",      \n",
    "        \"spark.yarn.am.memory\": \"16g\",     \n",
    "        \"spark.dynamicAllocation.enabled\": \"true\", \n",
    "        \"spark.task.cpus\": \"1\",          \n",
    "        \"spark.jars.packages.resolve.transitive\": \"true\",\n",
    "        \"spark.executor.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.driver.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.network.timeout\": \"1200s\",  \n",
    "        \"spark.rpc.askTimeout\": \"1200s\", \n",
    "        \"spark.executor.memoryOverhead\": \"16g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04fa74-e410-455e-ba84-919cbeefe66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "import catboost_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7407cc-8efe-4fe0-8003-0a06d020be06",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding a parameter tag\n",
    "cohort = 'cohort2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe237a-df8a-47fe-805d-1c91368c5f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 Paths\n",
    "s3_bucket = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/2_processed_datasets/{cohort}\"\n",
    "train_input_path = f\"{s3_bucket}/train\"\n",
    "test_input_path = f\"{s3_bucket}/test\"\n",
    "\n",
    "# Read processed train and test datasets from S3\n",
    "print(\"Reading train and test datasets...\")\n",
    "train_df = spark.read.parquet(train_input_path)\n",
    "test_df = spark.read.parquet(test_input_path)\n",
    "\n",
    "print(\"Train and test datasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b8757-0c37-4dc0-9337-410dc9616c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify output\n",
    "print(\"Train Dataframe Schema:\")\n",
    "train_df.printSchema()\n",
    "print(\"Test Dataframe Schema:\")\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe3820-8c8b-41a2-a92d-3331f7086a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoost Pool objects\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache or persist the Spark DataFrames before creating the Pool\n",
    "train_df = train_df.select(\"features\", \"label\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df = test_df.select(\"features\", \"label\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Create the CatBoost Pool objects\n",
    "train_pool = catboost_spark.Pool(train_df)\n",
    "test_pool = catboost_spark.Pool(test_df)\n",
    "\n",
    "# Confirm the DataFrames are cached/persisted\n",
    "print(train_df.storageLevel)\n",
    "print(test_df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d92489-4702-4c25-83d1-f129d1cbcbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seeds for different runs - 10 models\n",
    "seeds = [3, 24, 18, 17, 19, 11, 38, 74, 35, 90]\n",
    "\n",
    "# Start model number tracker\n",
    "model_num = 1\n",
    "\n",
    "# Loop to train and save models (10 runs for stable feature selection)\n",
    "for seed in seeds:\n",
    "    print(f\"Training model {model_num} with seed {seed}...\")\n",
    "    \n",
    "    # Initialize CatBoost Spark Classifier with the current seed\n",
    "    classifier = catboost_spark.CatBoostClassifier(randomSeed=seed)\n",
    "\n",
    "    # Train the model\n",
    "    model = classifier.fit(train_pool, evalDatasets=[test_pool])\n",
    "\n",
    "    # Define the path to save the Spark model, including the model number\n",
    "    spark_model_path = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/4_models/{cohort}/spark_model_{model_num}\"\n",
    "\n",
    "    # Save the Spark model (with metadata)\n",
    "    model.write().overwrite().save(spark_model_path)\n",
    "\n",
    "    print(f\"Spark model {model_num} with metadata saved to: {spark_model_path}\")\n",
    "    \n",
    "    # Clean up memory for next run\n",
    "    del classifier\n",
    "    del model\n",
    "    \n",
    "    # Increment the model number for the next run\n",
    "    model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9d3a5-085f-4609-a78e-acc8fc03f798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional Seeds for miscellaneous or missing runs\n",
    "seeds = [90]\n",
    "\n",
    "# Start model number tracker\n",
    "model_num = 10\n",
    "\n",
    "# Loop to train and save models (10 runs for stable feature selection)\n",
    "for seed in seeds:\n",
    "    print(f\"Training model {model_num} with seed {seed}...\")\n",
    "    \n",
    "    # Initialize CatBoost Spark Classifier with the current seed\n",
    "    classifier = catboost_spark.CatBoostClassifier(randomSeed=seed)\n",
    "\n",
    "    # Train the model\n",
    "    model = classifier.fit(train_pool, evalDatasets=[test_pool])\n",
    "\n",
    "    # Define the path to save the Spark model, including the model number\n",
    "    spark_model_path = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/4_models/{cohort}/spark_model_{model_num}\"\n",
    "\n",
    "    # Save the Spark model (with metadata)\n",
    "    model.write().overwrite().save(spark_model_path)\n",
    "\n",
    "    print(f\"Spark model {model_num} with metadata saved to: {spark_model_path}\")\n",
    "    \n",
    "    # Clean up memory for next run\n",
    "    del classifier\n",
    "    del model\n",
    "    \n",
    "    # Increment the model number for the next run\n",
    "    model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b8922-5a40-4728-ac0c-7803e78f49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.unpersist()\n",
    "test_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994ae6d-b4ba-4622-9bcb-cc119f29e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a01636119c44ea28ebc581481e89a45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "max_height": "500px",
       "overflow_y": "scroll"
      }
     },
     "3bd949c37ddd4d48baaae20936dd6fc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Spark Job Progress"
       },
       "children": [
        "IPY_MODEL_cd1e41387a694a46bc0feb3bc43b740e"
       ],
       "layout": "IPY_MODEL_7d4b92517e9b44c6969566f46f1867d9",
       "selected_index": null
      }
     },
     "4682619abf6741158e0833cb191ba05c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_0a01636119c44ea28ebc581481e89a45"
      }
     },
     "566fd5d0063e43e680c50fbb4a863d3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_fa059ada50bd44f0abfd975d70325ef1"
      }
     },
     "7d4b92517e9b44c6969566f46f1867d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9f84776f493a44eea35a4dd51f233853": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cd1e41387a694a46bc0feb3bc43b740e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4682619abf6741158e0833cb191ba05c"
       ],
       "layout": "IPY_MODEL_9f84776f493a44eea35a4dd51f233853"
      }
     },
     "fa059ada50bd44f0abfd975d70325ef1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
