{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5dedf-e7c6-476d-a86e-34eb13b5f870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"ai.catboost:catboost-spark_3.5_2.12:1.2.7\",\n",
    "        \"spark.executor.memory\": \"24g\",\n",
    "        \"spark.executor.cores\": \"4\",       \n",
    "        \"spark.driver.memory\": \"24g\",      \n",
    "        \"spark.yarn.am.memory\": \"4g\",     \n",
    "        \"spark.dynamicAllocation.enabled\": \"true\", \n",
    "        \"spark.task.cpus\": \"4\",          \n",
    "        \"spark.jars.packages.resolve.transitive\": \"true\",\n",
    "        \"spark.executor.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.driver.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.network.timeout\": \"1200s\",  \n",
    "        \"spark.rpc.askTimeout\": \"1200s\", \n",
    "        \"spark.executor.memoryOverhead\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04fa74-e410-455e-ba84-919cbeefe66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "import catboost_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7407cc-8efe-4fe0-8003-0a06d020be06",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding a parameter tag\n",
    "cohort = 'cohort1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe237a-df8a-47fe-805d-1c91368c5f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 Paths\n",
    "s3_bucket = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/2_processed_datasets/{cohort}\"\n",
    "train_input_path = f\"{s3_bucket}/train\"\n",
    "test_input_path = f\"{s3_bucket}/test\"\n",
    "\n",
    "# Read processed train and test datasets from S3\n",
    "print(\"Reading train and test datasets...\")\n",
    "train_df = spark.read.parquet(train_input_path)\n",
    "test_df = spark.read.parquet(test_input_path)\n",
    "\n",
    "print(\"Train and test datasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b8757-0c37-4dc0-9337-410dc9616c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify output\n",
    "print(\"Train Dataframe Schema:\")\n",
    "train_df.printSchema()\n",
    "print(\"Test Dataframe Schema:\")\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe3820-8c8b-41a2-a92d-3331f7086a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoost Pool objects\n",
    "train_pool = catboost_spark.Pool(train_df.select(\"features\", \"label\"))\n",
    "\n",
    "test_pool = catboost_spark.Pool(test_df.select(\"features\", \"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d92489-4702-4c25-83d1-f129d1cbcbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seeds for different runs\n",
    "seeds = [3, 19, 97, 11, 35, 90, 38, 74, 25, 974]\n",
    "\n",
    "# Start model number tracker\n",
    "model_num = 1\n",
    "\n",
    "# Loop to train and save models (10 runs for stable feature selection)\n",
    "for seed in seeds:\n",
    "    print(f\"Training model {model_num} with seed {seed}...\")\n",
    "    \n",
    "    # Initialize CatBoost Spark Classifier with the current seed\n",
    "    classifier = catboost_spark.CatBoostClassifier(randomSeed=seed)\n",
    "\n",
    "    # Train the model\n",
    "    model = classifier.fit(train_pool, evalDatasets=[test_pool])\n",
    "\n",
    "    # Define the path to save the Spark model, including the model number\n",
    "    spark_model_path = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/4_models/{cohort}/spark_model_{model_num}\"\n",
    "\n",
    "    # Save the Spark model (with metadata)\n",
    "    model.write().overwrite().save(spark_model_path)\n",
    "\n",
    "    print(f\"Spark model {model_num} with metadata saved to: {spark_model_path}\")\n",
    "    \n",
    "    # Increment the model number for the next run\n",
    "    model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994ae6d-b4ba-4622-9bcb-cc119f29e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
