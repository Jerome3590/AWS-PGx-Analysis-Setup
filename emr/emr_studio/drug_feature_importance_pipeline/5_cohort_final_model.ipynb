{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f5cae-768f-4259-aee2-12fddb82741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.memory\": \"24g\",\n",
    "        \"spark.executor.cores\": \"4\",       \n",
    "        \"spark.driver.memory\": \"24g\",      \n",
    "        \"spark.yarn.am.memory\": \"4g\",     \n",
    "        \"spark.dynamicAllocation.enabled\": \"true\", \n",
    "        \"spark.task.cpus\": \"4\",          \n",
    "        \"spark.jars.packages.resolve.transitive\": \"true\",\n",
    "        \"spark.executor.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.driver.extraJavaOptions\": \"--add-exports java.base/sun.net.util=ALL-UNNAMED\",\n",
    "        \"spark.network.timeout\": \"1200s\",  \n",
    "        \"spark.rpc.askTimeout\": \"1200s\", \n",
    "        \"spark.executor.memoryOverhead\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4e438-0805-4bb5-94b5-86634e179cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f1b01-0a85-47e2-a97e-2d6226704773",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "cohort_num = \"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550516a-4861-4d89-a653-5d311a93d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Paths for input data\n",
    "s3_bucket = f\"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/1_input_datasets/cohort{cohort_num}\"\n",
    "train_input_path = f\"{s3_bucket}/train\"\n",
    "test_input_path = f\"{s3_bucket}/test\"\n",
    "\n",
    "# Paths for feature importance CSV\n",
    "feature_importance_path = \"s3://pgx-repository/ade-risk-model/Step5_Time_to_Event_Model/5_feature_importances/cohort{cohort_num}/consolidated_feature_importances.csv\"\n",
    "\n",
    "# Read train and test datasets\n",
    "train_df = spark.read.parquet(train_input_path)\n",
    "test_df = spark.read.parquet(test_input_path)\n",
    "\n",
    "train_df.printSchema()\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e373701-5629-4bf8-9741-df21feb69634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read unique drug names from the CSV\n",
    "drug_filter = spark.read.option(\"header\", \"true\").csv(feature_importance_path)\n",
    "\n",
    "# Extract the list of unique drug names\n",
    "drug_names = drug_filter.select(\"drug_name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter the train DataFrame\n",
    "filtered_train_df = train_df.filter(\n",
    "    (col(\"standardized_drug_names\").isin(drug_names)) |  # Keep rows with important drug features\n",
    "    (col(\"label\") == 1) |                           # Keep rows with label == 1\n",
    "    (col(\"hospitalization\") == 1)                   # Keep rows with hospitalization == 1\n",
    ")\n",
    "\n",
    "# Filter the test DataFrame\n",
    "filtered_test_df = test_df.filter(\n",
    "    (col(\"standardized_drug_names\").isin(important_drug_names)) |  # Keep rows with important drug features\n",
    "    (col(\"label\") == 1) |                           # Keep rows with label == 1\n",
    "    (col(\"hospitalization\") == 1)                   # Keep rows with hospitalization == 1\n",
    ")\n",
    "\n",
    "# Paths for saving filtered datasets to S3\n",
    "filtered_train_output_path = f\"{s3_bucket}/filtered_train\"\n",
    "filtered_test_output_path = f\"{s3_bucket}/filtered_test\"\n",
    "\n",
    "# Save filtered DataFrames back to S3\n",
    "filtered_train_df.write.mode(\"overwrite\").parquet(filtered_train_output_path)\n",
    "filtered_test_df.write.mode(\"overwrite\").parquet(filtered_test_output_path)\n",
    "\n",
    "print(f\"Filtered train dataset saved to {filtered_train_output_path}\")\n",
    "print(f\"Filtered test dataset saved to {filtered_test_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9518e1-5d5e-4323-a10f-5529823c465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert Spark DataFrames to Pandas\n",
    "train_pandas_df = filtered_train_df.toPandas()\n",
    "test_pandas_df = filtered_test_df.toPandas()\n",
    "\n",
    "# Sort by 'mi_person_key' and 'drug_date'\n",
    "train_pandas = train_pandas_df.sort_values(by=['mi_person_key', 'drug_date'])\n",
    "test_pandas = test_pandas_df.sort_values(by=['mi_person_key', 'drug_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e874e5b-c06e-41d5-9c4c-40511e84707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "import shap\n",
    "\n",
    "# Preprocess the train and test data\n",
    "X_Train_sorted = train_pandas.drop(columns=['label'])\n",
    "Y = train_pandas['label']\n",
    "\n",
    "X_Test_sorted = test_pandas.drop(columns=['label'])\n",
    "y = test_pandas['label']\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 6),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 12),\n",
    "        \"iterations\": 500,\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"eval_metric\": 'Recall',\n",
    "        \"cat_features\": ['standardized_drug_name']  # Specify categorical features\n",
    "    }\n",
    "\n",
    "    # Create CatBoost pools with mi_person_key as group_id\n",
    "    train_pool = Pool(\n",
    "        data=X_Train_sorted.drop(columns=['drug_date']),\n",
    "        label=Y,\n",
    "        cat_features=['standardized_drug_name'],\n",
    "        group_id=X_Train_sorted['mi_person_key']\n",
    "    )\n",
    "    test_pool = Pool(\n",
    "        data=X_Test_sorted.drop(columns=['drug_date']),\n",
    "        label=y,\n",
    "        cat_features=['standardized_drug_name'],\n",
    "        group_id=X_Test_sorted['mi_person_key']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=test_pool, verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    preds = model.predict(X_Test_sorted.drop(columns=['drug_date']))\n",
    "    recall = recall_score(y, preds)\n",
    "    f1 = f1_score(y, preds)\n",
    "    return (recall + f1) / 2  # Combined metric\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_params = study.best_trial.params\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "train_pool = Pool(\n",
    "    data=X_Train_sorted.drop(columns=['drug_date']),\n",
    "    label=Y,\n",
    "    cat_features=['standardized_drug_name'],\n",
    "    group_id=X_Train_sorted['mi_person_key']\n",
    ")\n",
    "test_pool = Pool(\n",
    "    data=X_Test_sorted.drop(columns=['drug_date']),\n",
    "    label=y,\n",
    "    cat_features=['standardized_drug_name'],\n",
    "    group_id=X_Test_sorted['mi_person_key']\n",
    ")\n",
    "final_model.fit(train_pool, eval_set=test_pool, verbose=100)\n",
    "\n",
    "# Calculate SHAP values for the final model\n",
    "shap_values = final_model.get_feature_importance(type='ShapValues', data=test_pool)\n",
    "shap_df = pd.DataFrame(shap_values[:, :-1], columns=X_Test_sorted.drop(columns=['drug_date']).columns)\n",
    "\n",
    "# Group SHAP values for each level of standardized_drug_name\n",
    "level_shap_df = X_Test_sorted[['standardized_drug_name']].copy()\n",
    "level_shap_df['SHAP'] = shap_df['standardized_drug_name']\n",
    "\n",
    "# Aggregate SHAP values by drug levels\n",
    "level_importance = level_shap_df.groupby('standardized_drug_name')['SHAP'].mean().abs().sort_values(ascending=False)\n",
    "\n",
    "# Output the SHAP importance by individual levels\n",
    "print(\"SHAP importance by standardized_drug_name levels:\")\n",
    "print(level_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246f1e6-c958-4a64-b3ee-4a965f72ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "# Extract the best parameters from the Optuna study\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Add any additional fixed parameters needed for the final model\n",
    "best_params.update({\n",
    "    \"iterations\": 1000,            # Or the number of iterations you desire\n",
    "    \"boosting_type\": \"Ordered\",    # Keep it consistent with the Optuna runs\n",
    "    \"bootstrap_type\": \"MVS\",       # As used during the trial\n",
    "    \"early_stopping_rounds\": 100,  # Optional, if you want early stopping\n",
    "    \"eval_metric\": 'Recall'        # Consistent evaluation metric\n",
    "})\n",
    "\n",
    "cat_features = ['standardized_drug_name']\n",
    "group_id = 'mi_person_key'\n",
    "\n",
    "# Convert train and test datasets to CatBoost Pools\n",
    "final_x_train = X_Train_sorted.drop(['mi_person_key', 'drug_date'], axis=1)\n",
    "final_y_train = train_pandas['label']\n",
    "\n",
    "final_x_test = X_Test_sorted.drop(['mi_person_key', 'drug_date'], axis=1)\n",
    "final_y_test =  test_pandas['label']\n",
    "\n",
    "final_train_pool = Pool(final_x_train, final_y_train, cat_features=cat_features, group_id=group_id)\n",
    "final_test_pool = Pool(final_x_test, final_y_test, cat_features=cat_features, group_id=group_id)\n",
    "\n",
    "# Train the final CatBoost model with the best parameters\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "final_model.fit(\n",
    "    final_train_pool,\n",
    "    eval_set=final_test_pool,\n",
    "    early_stopping_rounds=100,  \n",
    "    verbose=100         \n",
    ")\n",
    "\n",
    "# Save the final model if needed\n",
    "final_model.save_model(f\"final_cohort{cohort_num}_model.cbm\")\n",
    "\n",
    "# Evaluate the model\n",
    "final_predictions = final_model.predict(final_test_pool.get_features())\n",
    "final_recall = recall_score(test_df['label'], final_predictions.round().astype(int))\n",
    "final_f1 = f1_score(test_df['label'], final_predictions.round().astype(int))\n",
    "\n",
    "print(f\"Final Model Recall: {final_recall}\")\n",
    "print(f\"Final Model F1 Score: {final_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11a5a7-44b7-4c81-b930-189cd3ba48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the best parameters from the Optuna study\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Add any additional fixed parameters needed for the final model\n",
    "best_params.update({\n",
    "    \"iterations\": 1000,            # Or the number of iterations you desire\n",
    "    \"boosting_type\": \"Ordered\",    # Keep it consistent with the Optuna runs\n",
    "    \"bootstrap_type\": \"MVS\",       # As used during the trial\n",
    "    \"early_stopping_rounds\": 100,  # Optional, if you want early stopping\n",
    "    \"eval_metric\": 'Recall'        # Consistent evaluation metric\n",
    "})\n",
    "\n",
    "# Prepare train and test datasets\n",
    "X_Train_sorted = train_pandas.drop(columns=['label'])\n",
    "Y = train_pandas['label']\n",
    "\n",
    "X_Test_sorted = test_pandas.drop(columns=['label'])\n",
    "y = test_pandas['label']\n",
    "\n",
    "# Create CatBoost Pools with group_id\n",
    "train_pool = Pool(\n",
    "    data=X_Train_sorted.drop(columns=['drug_date', 'mi_person_key']),\n",
    "    label=Y,\n",
    "    cat_features=['standardized_drug_name'],\n",
    "    group_id=X_Train_sorted['mi_person_key']\n",
    ")\n",
    "test_pool = Pool(\n",
    "    data=X_Test_sorted.drop(columns=['drug_date', 'mi_person_key']),\n",
    "    label=y,\n",
    "    cat_features=['standardized_drug_name'],\n",
    "    group_id=X_Test_sorted['mi_person_key']\n",
    ")\n",
    "\n",
    "# Train the CatBoost model\n",
    "model = CatBoostClassifier(**best_params)\n",
    "model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100,  verbose=100)\n",
    "\n",
    "# Compute SHAP interaction values\n",
    "shap_interaction_values = model.get_feature_importance(type='ShapInteractionValues', data=test_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11ae41-a7ba-4c6d-8b2c-6a9465186eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "# Save the final model if needed\n",
    "final_model.save_model(f\"final_cohort{cohort_num}_model.cbm\")\n",
    "\n",
    "# Evaluate the model\n",
    "final_predictions = final_model.predict(final_test_pool.get_features())\n",
    "final_recall = recall_score(test_df['label'], final_predictions.round().astype(int))\n",
    "final_f1 = f1_score(test_df['label'], final_predictions.round().astype(int))\n",
    "\n",
    "print(f\"Final Model Recall: {final_recall}\")\n",
    "print(f\"Final Model F1 Score: {final_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aec84a-901d-445e-a126-d9a600abe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Extract self-interaction values (diagonal)\n",
    "self_interactions = shap_interaction_values[:, 0, 0]  # Diagonal of SHAP interaction matrix\n",
    "\n",
    "# Combine with test data\n",
    "interaction_df = X_Test_sorted[['group_id', 'standardized_drug_name']].copy()\n",
    "interaction_df['Self_SHAP_Interaction'] = self_interactions\n",
    "\n",
    "# Filter out rows with zero SHAP contributions\n",
    "interaction_df_filtered = interaction_df[interaction_df['Self_SHAP_Interaction'] != 0]\n",
    "\n",
    "# Sort the DataFrame in descending order of SHAP values\n",
    "interaction_df_sorted = interaction_df_filtered.sort_values(\n",
    "    by='Self_SHAP_Interaction', ascending=False\n",
    ")\n",
    "\n",
    "# Display the top SHAP values\n",
    "print(\"Top SHAP self-interaction values sorted in descending order:\")\n",
    "print(interaction_df_sorted)\n",
    "\n",
    "# Save the sorted SHAP DataFrame to a CSV\n",
    "interaction_df_sorted.to_csv(\"drugs_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee04556-e815-4ef8-9904-c29bbd6273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Define AWS S3 bucket and file details\n",
    "bucket_name = 'pgx-repository'  \n",
    "file_name = 'drugs_final.csv'  \n",
    "s3_key = f\"ade-risk-model/Step5_Time_to_Event_Model/5_feature_importances/cohort{cohort_num}/{file_name}\"  \n",
    "\n",
    "# Initialize an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the file\n",
    "try:\n",
    "    s3_client.upload_file(file_name, bucket_name, s3_key)\n",
    "    print(f\"File '{file_name}' successfully uploaded to S3 bucket '{bucket_name}' as '{s3_key}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155ddd8-b351-481b-b341-79052cca6c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
